# Numeric vectors

## `seq()`ences of numbers

[2.1.4. Generating arithmetic progressions with seq and `:`](https://deepr.gagolewski.com/chapter/120-numeric.html#generating-arithmetic-progressions-with-seq-and)

> *Take a look at the manual page of seq_along and seq_len and determine whether we can do without them, having seq at hand.*

So the two other functions cover cases that are handled by a single argument to `seq()`.

```{r seq}
# Sequence up to length 5
seq(length.out = 5)
seq_len(5)

# Sequence positions of something of length 10
seq(along.with = 50:59)
seq_along(50:59)
```

Same results, pretty straightforward; clearly we *could* do without them. On the odd occasion I want to do positional iteration over a vector, I do tend to use `seq_along()`, but I could live with the extra 7 characters.

```{r seq_along}
count <- c(5, 6, 7, 8)

# I sometimes have a construction a little like this
for (index in seq_along(count)) {
  print(count[[index]])
}
```

## `scan()` in data

[2.1.6. Reading data with scan](https://deepr.gagolewski.com/chapter/120-numeric.html#reading-data-with-scan)

> *Read the help page about scan. Take note of the following formal arguments and their meaning: dec, sep, what, comment.char, and na.strings.*

`scan()` reads data into a vector from file. It has *22* Arguments! Alright:

-   **dec** - Specifies the character used for decimals separators. The default is the point `"."`. I imagine the most common alternative would be the comma `","`, which is a standard in much of the world as well.
-   **sep** - Specifies a character to be interpeted as a delimiter between data. You could pass `sep = ","` for reading in a Comma Separated Values file. The default to the argument is `""` but the default behaviour is to split on whitespace.
-   **what** - Specify the `typeof()` the data being read in.
-   **comment.char** - If I'm understanding correctly, this is for reading in a code file and specifying a character to be interpreted as denoting the start of a comment. So you would pass `"#"` for an R file. It only takes a single-character though, so I'm not sure it could handle some other common structure, particularly for multi-lines.
-   **na.strings** - A character vector of strings to be interpreted as `NA` values by R.

## Start making a `plot()`

> *Somewhat misleadingly (and for reasons that will become apparent later), the documentation of plot can be accessed by calling help("plot.default"). Read about, and experiment with, different values of the main, xlab, ylab, type, col, pch, cex, lty, and lwd arguments. More plotting routines will be discussed in Chapter 13.*

Let's build up a plot with them to try it out.

**main, xlab, ylab**

A title for the plot, its x-axis and y-axis, respectively.

```{r labels}
plot(
  0:25,
  main = "This is what my plot's all about",
  xlab = "This axis goes along",
  ylab = "This axis goes up"
)
```

**pch**

See `help("points")`. Give a number to specify the symbol to be used for the points, e.g. a solid circle or filled diamond. Numbers 0:25 have some common graphical symbols and 32:127 have other ASCII characters. Check them out:

```{r pch}
# Even specify points on the graph with a vector
plot(
  0:25,
  pch = 0:25
)
```

**cex**

Couldn't quite make them out? Specify a number by which to scale points.

```{r cex}
# Twice the size, please!
plot(
  0:25,
  pch = 0:25,
  cex = 2
)
```

**type**

The kind of things to be drawn, including points or lines, though curiously specified with a single-character only like `"p"` or `"l"`.

```{r type}
# Let's try a line instead
plot(
  0:25,
  type = "l"
)
```

**col - Colour**

The colours used for the lines and points of the plot.

```{r col}
# Add a hint of blue
plot(
  0:25,
  type = "l",
  col = "blue"
)
```

**lty - line-type**

Had to dig into `help("par")`. This is the line-type, you can change it from the default solid line.

```{r lty}
# We can change the line type
plot(
  0:25,
  type = "l",
  col = "blue",
  lty = "dotted",
)
```

**lwd - line-width**

Finally, although far from final out of all the graphical options, we can change the line width much like we can scale the symbol size.

```{r lwd}
# Triple the width!
plot(
  0:25,
  type = "l",
  col = "blue",
  lty = "dotted",
  lwd = 3
)
```

## Exponentials and natural logs are opposites

[2.3.3. Natural exponential function and logarithm](https://deepr.gagolewski.com/chapter/120-numeric.html#natural-exponential-function-and-logarithm)

> *Commonly, a logarithmic scale is used for variables that grow rapidly when expressed as functions of each other*

I can see how it might be useful, though I'm curious to what actual examples might be (I have no statistics knowledge). I'll just include the very nice example provided.

```{r x-example}
x <- seq(0, 10, length.out=1001)
```

Noting that you can design a grid layout for graphics by setting the graphical parameters `mfrow` and `mfcol` with `par()`.

```{r log-scale}
par(mfrow=c(1, 2))
plot(x, exp(x), type="l", main = "linear-scale y-axis")
plot(x, exp(x), type="l", log="y", main = "log-scale y-axis")  
```

> *Let's highlight that* $e^{x}$ *on the log-scale is nothing more than a straight line*

I understand it as natural log (i.e. `log()` with its default `base = exp(1)` is the opposite function to computing the exponential. It may be helpful to compare them side-by-side.

```{r exp-and-log}
par(mfrow=c(1, 2))
plot(x, exp(x), type="l", main = "exponential")
plot(x, log(x), type="l", main = "natural log") 
```

Or more directly see that they cancel each other out:

```{r exp-log}
exp(log(10))
```

## Draw a `hist()`gram of probability distributions

[2.3.4. Probability distributions](https://deepr.gagolewski.com/chapter/120-numeric.html#probability-distributions)

> *A call to hist(x) draws a histogram, which can serve as an estimator of the underlying continuous probability density function of a given sample; see [Figure 2.3](https://deepr.gagolewski.com/chapter/120-numeric.html#id27) for an illustration.*
>
> *Draw a histogram of some random samples of different sizes n from the following distributions:*
>
> -   ***rnorm**(n, µ, σ) -- normal* $N(μ, σ)$ *with expected values* $μ ∈ {−1,0,5}$ *(i.e.,* $μ$ *being equal to either* $−1, 0$, or $5$*; read "*$∈$*" as "belongs to the given set" or "in") and standard deviations* $σ ∈ {0.5,1,5}$*;*
>
> -   ***runif**(n, a, b) -- uniform* $U(a,b)$ *on the interval* $(a,b)$ *with* $a=0$ *and* $b=1$ *as well as* $a=−1 and b=1$*;*
>
> -   **rbeta**(n, α, β) -- beta $B(α,β)$ with $α,β ∈ {0.5,1,2}$;
>
> -   **rexp**(n, λ) -- exponential $E(λ)$ with rates $λ ∈ {0.5,1,10}$;
>
> *Moreover, read about and play with the breaks, main, xlab, ylab, xlim, ylim, and col parameters; see help("hist").*

```{r seed}
set.seed(12345) 
```

Uh-oh, time to learn some maths (nice, actually). Here we go:

```{r rnorm}
set_sd <- 5

hist(
  rnorm(10000, -1, set_sd), 
  main = "normal distribution",
  xlim = c(-3 * set_sd, 3 * set_sd),
  probability = TRUE
)

x <- seq(-3 * set_sd, 3 * set_sd, length.out=101) 
lines(x, dnorm(x,-1, set_sd), lwd=2) 
```

So, as I understand it here the mean, median and mode[^1] of the distribution should approximate to the second argument, $-1$. Then just over $2/3$ should be within $5$ (the standard deviation) either side of that, and *almost* all should be with $15$ (3 standard deviations). I used `xlim` to cut the axis to $-15$ to $15$, but you can see the edge of remaining bins creep in. Aha, like I said, I don't know statistics. 

I need to go on a tangent and finally find out what standard deviation is.

```{r sd}
body(sd)
```
The square root of the variance, hey? The body of `var` isn't much help because it calls some C or C++ function. Looking it up, variance is a measure of how dispersed a data set is. Standard deviation is too, but perhaps sits at a more convenient range of values for doing other things with the value. Variance is the mean of the squares of the distance from the mean. Ah, I'm going to need to try that to figure it out. 

```{r var}
var(1:100)

my_var <- function(x) mean((x - mean(x))^2)

my_var(1:100)
```
I got close, but not quite right, hmmm. Another definition specifies taking the sum of the squares of the distance by mean and dividing by $n - 1$ where n is the number of numbers. So let's try that instead of the final mean (which would be the sum divided by n).

```{r var2}
my_var2 <- function(x) sum((x - mean(x))^2) / (length(x) - 1)

my_var2(1:100)
```

Aha! But there does seem to be two definitions, I'll have to revisit that. Anyway, so then we just need to take the square root:

```{r my_sd}
my_sd <- function(x) sqrt(my_var2(x))

my_sd(1:100)

sd(1:100)
```

Neat. Okay, it makes sense to me that distance from the mean would be used to present deviation, but I'm not quite what the rest is for. I can see that taking the square resolves negative deviation and the square root at the end somewhat rebalances it. If that were the only goal, then why wouldn't the absolute differences be used? Something like:

```{r abs_dev}
absolute_deviation <- function(x) {
  mean(abs(x - mean(x)))
}

absolute_deviation(1:100)
```
Maybe that could work? Well, apparently it is a thing - [Average Absolute Deviation](https://en.wikipedia.org/wiki/Average_absolute_deviation), but I suppose for now I just have to accept that standard deviation is just that, standard.

[^1]: I found that while R does provide `mean()` and `median()`, the function `mode()` is more akin to `typeof()` in providing the data type/mode. There is not a standard in-built function for the statistical mode.

```{r runif}
unif_min <- -1
unif_max <- 1

hist(
  runif(10000, unif_min, unif_max),
  col = c("red", "orange", "green", "yellow", "blue", "purple", "violet"),
  breaks = 7 * 7,
  main = "a uniform rainbow",
  probability = TRUE
)

x <- seq(unif_min, unif_max, length.out = 10)
lines(x, dunif(x, unif_min, unif_max), lwd = 2)
```

He *did* say to play with `col` (colours) and `breaks` (number of bins). I note that `n = 10000` seems enough to really see the expected distribution take shape, but it's not so clear at e.g. `n = 1000` and especially `n = 100`. Though even at 10000, as you get more breaks/bins like this, the variation becomes more visible again.

```{r rbeta}
x <- seq(0, 1, length.out = 100)

par(mfrow = c(2, 2))
hist(rbeta(10000, 0.5, 0.5), main = "0.5s are high at the edges", probability = TRUE)
lines(x, dbeta(x, 0.5, 0.5), lwd = 2)

hist(rbeta(10000, 1, 1), main = "1s are roughly uniform", probability = TRUE)
lines(x, dbeta(x, 1, 1), lwd = 2)

hist(rbeta(10000, 2, 2), main = "2s are low at the edges", probability = TRUE)
lines(x, dbeta(x, 2, 2), lwd = 2)
hist(rbeta(10000, 0.5, 2), main = "we can go from one extreme to the other", probability = TRUE)
lines(x, dbeta(x, 0.5, 2), lwd = 2)
```

I'm not really sure what's going on here. Roughly I understand that it produces a distribution between 0 and 1, where `shape1` perhaps has more influence closer to 0 and `shape2` closer to 1. The higher the number, the lower the density at its respective end of the distribution.

```{r rexp}
rate <- 10

hist(rexp(10000, rate), main = "exponential distribution", probability = TRUE)

x <- seq(0, 1, length.out = 100)
lines(x, dexp(x, rate), lwd = 2)
```

As we saw from the plotting `exp()` earlier, the majority sit at a very low value, until it reaches the point of rapid growth, where a rapidly decreasing amount of numbers can be found at each range on the way up. Plotting the density seems to give a horizontally flipped shape. The rate ($λ$) controls how rapid it grows.

## Binomial distribution

Marek gives a really nice example here of six-sided dice rolls to explain both binomial distribution and the different distribution function prefixes R uses; d\*[^2], b\*, q\* and r\*. So, binomial distribution gives us the probability distribution of successes in a sequence of tests/trials. As per the example, this could be a sequence of dice rolls with a particular face result counted as a success.

[^2]: Amusing to discover `df()` is a distribution function in R, after seeing so many dataframes borrowing that object name.

```{r dbinom}
n = 12 # Number of trials
p = 1/6 # Probability of success of rolling a particular face in a single roll

# See how likely each number of success is, from the minimum 0 times to the max n times
round(dbinom(0:n, n, p), 2)
```
It shows 2 successes is just about the most likely (as we started from 0). Then we can check how likely it is to see a certain number of successes or fewer.
```{r pbinom}
round(pbinom(0:n, n, p), 2)
```
It's very unlikely to roll more than 4 of the same face, so we see 96% probability to roll 4 or less times. By 6 rolls, it's indistinguishable from guaranteed (at 2 decimal places), which - yeah - does make sense for a 1/6 chance. Sort of the inverse view, we can also check the smallest number of (maximum?) successes for a certain likelihood. Here we use the q (quantile) function to check by decile:
```{r qbinom}
qbinom(seq(0, 1, length.out = 11), n, p)
```
So to reach at least 70% or 80% likelihood of rolling our number of successes or lower, that number would have to be 3, as per the 0.87 figure from the probability distribution. I am half following what's going on 😅. Back to more familiar territory of random number generation, we can see how many successes we get across multiple sets of n attempts:
```{r rbinom}
rbinom(30, n, p)
```