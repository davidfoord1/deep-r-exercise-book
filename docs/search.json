[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Deep R Programming Exercise Book",
    "section": "",
    "text": "Preface\nHi, I’m David. At the time of writing, a junior data analyst who started using R for data pipelines and analysis in the middle of last year. This is my attempt at, and way of holding myself accountable for, working my way through the book:"
  },
  {
    "objectID": "index.html#deep-r-programming",
    "href": "index.html#deep-r-programming",
    "title": "Deep R Programming Exercise Book",
    "section": "Deep R Programming",
    "text": "Deep R Programming\nby Marek Gagolewski\nIt’s an open-access introductory textbook, which goes into quite some depth about the data structures and working of R. I’ve actually already read a lot of it in paper copy and rather enjoyed it, but now want to go back and work through the exercises, slowly but surely.\nAims, scope, and design philosophy\nHaving been introduced to R through the tidyverse, I am among the cohort described by Marek as:\n\nisolated from base R through a thick layer of popular third-party packages that introduce an overwhelming number of functions\n\nI’m feeling keen to get to grips with the capabilities of R itself, and understanding a bit more about what’s going on under-the-hood."
  },
  {
    "objectID": "index.html#exercise-book-structure",
    "href": "index.html#exercise-book-structure",
    "title": "Deep R Programming Exercise Book",
    "section": "Exercise Book Structure",
    "text": "Exercise Book Structure\nI will organise this book by chapters and by exercises. Note that the exercises are numbered in order relative to their respective chapter e.g. chapter.exercise_number, not the subsection of the book. For example the first exercise 1.1 is found in the book subsection 1.2.3. As the HTML version doesn’t provide a link to the exercise blocks themselves, I will link to the subsection where they can be found.\nThe exercises also share numbering with examples, so there may or may not always be a task. My heading numbering will match the exercise/example numbering and in most cases I will provide a link to the section of the book the exercise is featured in.\n\nExercises will be quoted in block quotes and italics like this"
  },
  {
    "objectID": "index.html#license",
    "href": "index.html#license",
    "title": "Deep R Programming Exercise Book",
    "section": "License",
    "text": "License\nDeep R Programming is published under the license is published under the Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License (CC BY-NC-ND 4.0)."
  },
  {
    "objectID": "intro.html#batch-mode",
    "href": "intro.html#batch-mode",
    "title": "1  Introduction",
    "section": "1.1 Batch mode",
    "text": "1.1 Batch mode\n1.2.3. Batch mode: Working with R scripts\n\nIn your favourite text editor (e.g., Notepad++, Kate, vi, Emacs, RStudio, or VSCodium), create a file named test.R. Write a few calls to the cat function. Then, execute this script from the terminal through Rscript.\n\nMy favourite way to use the terminal at the moment is opening Git Bash within RStudio so that I can execute things immediately at the project location. This book introduced me to the Rscript command, it’s a pretty neat way of doing things. I can indeed cat() “cat” by running:\nRscript R/1.1_cat-cat.R"
  },
  {
    "objectID": "intro.html#report-generation",
    "href": "intro.html#report-generation",
    "title": "1  Introduction",
    "section": "1.2 Report generation",
    "text": "1.2 Report generation\n1.2.4. Weaving: Automatic report generation\nI’ll sidestep this one, which is about using {knitr}, on top of which I imagine much quarto functionality is built, to render a report. The example here does demonstrate something else about batch mode; you can execute an R expression in-line as well.\nRscript -e 'cat(\"cat\")'\nI’m relying on quarto to weave.\nquarto render"
  },
  {
    "objectID": "intro.html#notebooks",
    "href": "intro.html#notebooks",
    "title": "1  Introduction",
    "section": "1.3 Notebooks",
    "text": "1.3 Notebooks\n1.2.5. Semi-interactive modes (Jupyter Notebooks, sending code to the associated R console, etc.)\nI’m editing a Quarto document, intro.qmd in RStudio, a notebook in the sense of interweaving code chunks and prose. I’ll kindly ask you to take my word for it that when I press CTRL + ENTER against a line of code in the markdown document, it does indeed get sent to console for execution."
  },
  {
    "objectID": "intro.html#section",
    "href": "intro.html#section",
    "title": "1  Introduction",
    "section": "1.4 ",
    "text": "1.4 \nThere is an “Example 1.4”, rather than an exercise, it’s just talking about notebooks."
  },
  {
    "objectID": "intro.html#getting-help",
    "href": "intro.html#getting-help",
    "title": "1  Introduction",
    "section": "1.5 Getting Help",
    "text": "1.5 Getting Help\n1.4. Getting help\n\nSight (without going into detail) the manual on the length function by calling help(“length”). Note that most help pages are structured as follows:\nHeader: package:base means that the function is a base one (see Section 7.3.1 for more details on the R package system);\n\nTitle;\nDescription: a short description of what the function does;\nUsage: the list of formal arguments (parameters) to the function;\nArguments: the meaning of each formal argument explained;\nDetails: technical information;\nValue: return value explained;\nReferences: further reading;\nSee Also: links to other help pages;\nExamples: R code that is worth inspecting.\n\n\nBeing able to access all the necessary documentation in-situ in the IDE is fantastic, I really appreciate how much the R ecosystem is set up around this kind of documentation. One thing that seems a little loosey-goosey are around the types of the Arguments and the Value, they’re not always specified and coercion or not handled in different ways. I suppose that’s to be expected, not working in a strongly typed language.\nThis section introduced me to help.search() for topic searches, which is great. I will say I’m not quite so keen on relying on help() all the time as Marek is. It’s very help()ful for a quick lookup, but I don’t find it always necessarily covers all details or with total accuracy. Inevitably when issues arise I do wander over to “G**gle” and StackOverflow.\nOn top of that, help() works for checking a single function, but if I want to explore a package, its function reference and related articles, a website usually has a much nicer user experience. I’d rather explore his stringi package via its website stringi.gagolewski.com, for instance."
  },
  {
    "objectID": "intro.html#most-important-atomic-vectors",
    "href": "intro.html#most-important-atomic-vectors",
    "title": "1  Introduction",
    "section": "1.6 Most important atomic vectors",
    "text": "1.6 Most important atomic vectors\n1.5 Exercises\n\nWhat are the three most important types of atomic vectors?\n\nAlright, now we’re talking. As discussed in 1.3. Atomic vectors at a glance, the three key types are character, numeric and logical:\n\nstrings &lt;- c(\"strings\", \"like\", \"these\")\nclass(strings)\n\n[1] \"character\"\n\nnumbers &lt;- c(5, 6, 7, 8)\nclass(numbers)\n\n[1] \"numeric\"\n\nbooleans &lt;- c(TRUE, FALSE, TRUE)\nclass(booleans)\n\n[1] \"logical\""
  },
  {
    "objectID": "intro.html#classification-of-atomic-vectors",
    "href": "intro.html#classification-of-atomic-vectors",
    "title": "1  Introduction",
    "section": "1.7 Classification of atomic vectors",
    "text": "1.7 Classification of atomic vectors\n\nAccording to the classification of the R data types we introduced in the previous chapter, are atomic vectors basic or compound types?\n\nSee Classification of R data types and book structure.\nIt didn’t go into much detail about what makes a type basic as opposed to compound, stating that compound types wrap around the basic types and might behave differently because, well, because functions might treat them differently.\nNonetheless, atomic vectors, which are sequences of values of one type, are grouped as basic types. It’s a reasonable enough grouping, though numeric vectors in particular I could see being described as compound in a different context.\n\nintegers &lt;- c(5L, 6L, 7L, 8L)\nclass(integers)\n\n[1] \"integer\"\n\nis.numeric(integers)\n\n[1] TRUE\n\ndoubles &lt;- c(1, 2, 3, 4)\nclass(doubles)\n\n[1] \"numeric\"\n\nis.numeric(doubles)\n\n[1] TRUE\n\n\nSame class, different type, but I guess we’ll get on to that in the next chapter…"
  },
  {
    "objectID": "numeric.html#seqences-of-numbers",
    "href": "numeric.html#seqences-of-numbers",
    "title": "2  Numeric vectors",
    "section": "2.1 seq()ences of numbers",
    "text": "2.1 seq()ences of numbers\n2.1.4. Generating arithmetic progressions with seq and :\n\nTake a look at the manual page of seq_along and seq_len and determine whether we can do without them, having seq at hand.\n\nSo the two other functions cover cases that are handled by a single argument to seq().\n\n# Sequence up to length 5\nseq(length.out = 5)\n\n[1] 1 2 3 4 5\n\nseq_len(5)\n\n[1] 1 2 3 4 5\n\n# Sequence positions of something of length 10\nseq(along.with = 50:59)\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\nseq_along(50:59)\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\n\nSame results, pretty straightforward; clearly we could do without them. On the odd occasion I want to do positional iteration over a vector, I do tend to use seq_along(), but I could live with the extra 7 characters.\n\ncount &lt;- c(5, 6, 7, 8)\n\n# I sometimes have a construction a little like this\nfor (index in seq_along(count)) {\n  print(count[[index]])\n}\n\n[1] 5\n[1] 6\n[1] 7\n[1] 8"
  },
  {
    "objectID": "numeric.html#scan-in-data",
    "href": "numeric.html#scan-in-data",
    "title": "2  Numeric vectors",
    "section": "2.2 scan() in data",
    "text": "2.2 scan() in data\n2.1.6. Reading data with scan\n\nRead the help page about scan. Take note of the following formal arguments and their meaning: dec, sep, what, comment.char, and na.strings.\n\nscan() reads data into a vector from file. It has 22 Arguments! Alright:\n\ndec - Specifies the character used for decimals separators. The default is the point \".\". I imagine the most common alternative would be the comma \",\", which is a standard in much of the world as well.\nsep - Specifies a character to be interpeted as a delimiter between data. You could pass sep = \",\" for reading in a Comma Separated Values file. The default to the argument is \"\" but the default behaviour is to split on whitespace.\nwhat - Specify the typeof() the data being read in.\ncomment.char - If I’m understanding correctly, this is for reading in a code file and specifying a character to be interpreted as denoting the start of a comment. So you would pass \"#\" for an R file. It only takes a single-character though, so I’m not sure it could handle some other common structure, particularly for multi-lines.\nna.strings - A character vector of strings to be interpreted as NA values by R."
  },
  {
    "objectID": "numeric.html#start-making-a-plot",
    "href": "numeric.html#start-making-a-plot",
    "title": "2  Numeric vectors",
    "section": "2.3 Start making a plot()",
    "text": "2.3 Start making a plot()\n\nSomewhat misleadingly (and for reasons that will become apparent later), the documentation of plot can be accessed by calling help(“plot.default”). Read about, and experiment with, different values of the main, xlab, ylab, type, col, pch, cex, lty, and lwd arguments. More plotting routines will be discussed in Chapter 13.\n\nLet’s build up a plot with them to try it out.\nmain, xlab, ylab\nA title for the plot, its x-axis and y-axis, respectively.\n\nplot(\n  0:25,\n  main = \"This is what my plot's all about\",\n  xlab = \"This axis goes along\",\n  ylab = \"This axis goes up\"\n)\n\n\n\n\npch\nSee help(\"points\"). Give a number to specify the symbol to be used for the points, e.g. a solid circle or filled diamond. Numbers 0:25 have some common graphical symbols and 32:127 have other ASCII characters. Check them out:\n\n# Even specify points on the graph with a vector\nplot(\n  0:25,\n  pch = 0:25\n)\n\n\n\n\ncex\nCouldn’t quite make them out? Specify a number by which to scale points.\n\n# Twice the size, please!\nplot(\n  0:25,\n  pch = 0:25,\n  cex = 2\n)\n\n\n\n\ntype\nThe kind of things to be drawn, including points or lines, though curiously specified with a single-character only like \"p\" or \"l\".\n\n# Let's try a line instead\nplot(\n  0:25,\n  type = \"l\"\n)\n\n\n\n\ncol - Colour\nThe colours used for the lines and points of the plot.\n\n# Add a hint of blue\nplot(\n  0:25,\n  type = \"l\",\n  col = \"blue\"\n)\n\n\n\n\nlty - line-type\nHad to dig into help(\"par\"). This is the line-type, you can change it from the default solid line.\n\n# We can change the line type\nplot(\n  0:25,\n  type = \"l\",\n  col = \"blue\",\n  lty = \"dotted\",\n)\n\n\n\n\nlwd - line-width\nFinally, although far from final out of all the graphical options, we can change the line width much like we can scale the symbol size.\n\n# Triple the width!\nplot(\n  0:25,\n  type = \"l\",\n  col = \"blue\",\n  lty = \"dotted\",\n  lwd = 3\n)"
  },
  {
    "objectID": "numeric.html#exponentials-and-natural-logs-are-opposites",
    "href": "numeric.html#exponentials-and-natural-logs-are-opposites",
    "title": "2  Numeric vectors",
    "section": "2.4 Exponentials and natural logs are opposites",
    "text": "2.4 Exponentials and natural logs are opposites\n2.3.3. Natural exponential function and logarithm\n\nCommonly, a logarithmic scale is used for variables that grow rapidly when expressed as functions of each other\n\nI can see how it might be useful, though I’m curious to what actual examples might be (I have no statistics knowledge). I’ll just include the very nice example provided.\n\nx &lt;- seq(0, 10, length.out=1001)\n\nNoting that you can design a grid layout for graphics by setting the graphical parameters mfrow and mfcol with par().\n\npar(mfrow=c(1, 2))\nplot(x, exp(x), type=\"l\", main = \"linear-scale y-axis\")\nplot(x, exp(x), type=\"l\", log=\"y\", main = \"log-scale y-axis\")  \n\n\n\n\n\nLet’s highlight that \\(e^{x}\\) on the log-scale is nothing more than a straight line\n\nI understand it as natural log (i.e. log() with its default base = exp(1) is the opposite function to computing the exponential. It may be helpful to compare them side-by-side.\n\npar(mfrow=c(1, 2))\nplot(x, exp(x), type=\"l\", main = \"exponential\")\nplot(x, log(x), type=\"l\", main = \"natural log\") \n\n\n\n\nOr more directly see that they cancel each other out:\n\nexp(log(10))\n\n[1] 10"
  },
  {
    "objectID": "numeric.html#draw-a-histgram-of-probability-distributions",
    "href": "numeric.html#draw-a-histgram-of-probability-distributions",
    "title": "2  Numeric vectors",
    "section": "2.5 Draw a hist()gram of probability distributions",
    "text": "2.5 Draw a hist()gram of probability distributions\n2.3.4. Probability distributions\n\nA call to hist(x) draws a histogram, which can serve as an estimator of the underlying continuous probability density function of a given sample; see Figure 2.3 for an illustration.\nDraw a histogram of some random samples of different sizes n from the following distributions:\n\nrnorm(n, µ, σ) – normal \\(N(μ, σ)\\) with expected values \\(μ ∈ {−1,0,5}\\) (i.e., \\(μ\\) being equal to either \\(−1, 0\\), or \\(5\\); read “\\(∈\\)” as “belongs to the given set” or “in”) and standard deviations \\(σ ∈ {0.5,1,5}\\);\nrunif(n, a, b) – uniform \\(U(a,b)\\) on the interval \\((a,b)\\) with \\(a=0\\) and \\(b=1\\) as well as \\(a=−1 and b=1\\);\nrbeta(n, α, β) – beta \\(B(α,β)\\) with \\(α,β ∈ {0.5,1,2}\\);\nrexp(n, λ) – exponential \\(E(λ)\\) with rates \\(λ ∈ {0.5,1,10}\\);\n\nMoreover, read about and play with the breaks, main, xlab, ylab, xlim, ylim, and col parameters; see help(“hist”).\n\n\nset.seed(12345) \n\nUh-oh, time to learn some maths (nice, actually). Here we go:\n\nset_sd &lt;- 5\n\nhist(\n  rnorm(10000, -1, set_sd), \n  main = \"normal distribution\",\n  xlim = c(-3 * set_sd, 3 * set_sd),\n  probability = TRUE\n)\n\nx &lt;- seq(-3 * set_sd, 3 * set_sd, length.out=101) \nlines(x, dnorm(x,-1, set_sd), lwd=2) \n\n\n\n\nSo, as I understand it here the mean, median and mode1 of the distribution should approximate to the second argument, \\(-1\\). Then just over \\(2/3\\) should be within \\(5\\) (the standard deviation) either side of that, and almost all should be with \\(15\\) (3 standard deviations). I used xlim to cut the axis to \\(-15\\) to \\(15\\), but you can see the edge of remaining bins creep in. Aha, like I said, I don’t know statistics.\nI need to go on a tangent and finally find out what standard deviation is.\n\nbody(sd)\n\nsqrt(var(if (is.vector(x) || is.factor(x)) x else as.double(x), \n    na.rm = na.rm))\n\n\nThe square root of the variance, hey? The body of var isn’t much help because it calls some C or C++ function. Looking it up, variance is a measure of how dispersed a data set is. Standard deviation is too, but perhaps sits at a more convenient range of values for doing other things with the value. Variance is the mean of the squares of the distance from the mean. Ah, I’m going to need to try that to figure it out.\n\nvar(1:100)\n\n[1] 841.6667\n\nmy_var &lt;- function(x) mean((x - mean(x))^2)\n\nmy_var(1:100)\n\n[1] 833.25\n\n\nI got close, but not quite right, hmmm. Another definition specifies taking the sum of the squares of the distance by mean and dividing by \\(n - 1\\) where n is the number of numbers. So let’s try that instead of the final mean (which would be the sum divided by n).\n\nmy_var2 &lt;- function(x) sum((x - mean(x))^2) / (length(x) - 1)\n\nmy_var2(1:100)\n\n[1] 841.6667\n\n\nAha! But there does seem to be two definitions, I’ll have to revisit that. Anyway, so then we just need to take the square root:\n\nmy_sd &lt;- function(x) sqrt(my_var2(x))\n\nmy_sd(1:100)\n\n[1] 29.01149\n\nsd(1:100)\n\n[1] 29.01149\n\n\nNeat. Okay, it makes sense to me that distance from the mean would be used to present deviation, but I’m not quite what the rest is for. I can see that taking the square resolves negative deviation and the square root at the end somewhat rebalances it. If that were the only goal, then why wouldn’t the absolute differences be used? Something like:\n\nabsolute_deviation &lt;- function(x) {\n  mean(abs(x - mean(x)))\n}\n\nabsolute_deviation(1:100)\n\n[1] 25\n\n\nMaybe that could work? Well, apparently it is a thing - Average Absolute Deviation, but I suppose for now I just have to accept that standard deviation is just that, standard.\n\nunif_min &lt;- -1\nunif_max &lt;- 1\n\nhist(\n  runif(10000, unif_min, unif_max),\n  col = c(\"red\", \"orange\", \"green\", \"yellow\", \"blue\", \"purple\", \"violet\"),\n  breaks = 7 * 7,\n  main = \"a uniform rainbow\",\n  probability = TRUE\n)\n\nx &lt;- seq(unif_min, unif_max, length.out = 10)\nlines(x, dunif(x, unif_min, unif_max), lwd = 2)\n\n\n\n\nHe did say to play with col (colours) and breaks (number of bins). I note that n = 10000 seems enough to really see the expected distribution take shape, but it’s not so clear at e.g. n = 1000 and especially n = 100. Though even at 10000, as you get more breaks/bins like this, the variation becomes more visible again.\n\nx &lt;- seq(0, 1, length.out = 100)\n\npar(mfrow = c(2, 2))\nhist(rbeta(10000, 0.5, 0.5), main = \"0.5s are high at the edges\", probability = TRUE)\nlines(x, dbeta(x, 0.5, 0.5), lwd = 2)\n\nhist(rbeta(10000, 1, 1), main = \"1s are roughly uniform\", probability = TRUE)\nlines(x, dbeta(x, 1, 1), lwd = 2)\n\nhist(rbeta(10000, 2, 2), main = \"2s are low at the edges\", probability = TRUE)\nlines(x, dbeta(x, 2, 2), lwd = 2)\nhist(rbeta(10000, 0.5, 2), main = \"we can go from one extreme to the other\", probability = TRUE)\nlines(x, dbeta(x, 0.5, 2), lwd = 2)\n\n\n\n\nI’m not really sure what’s going on here. Roughly I understand that it produces a distribution between 0 and 1, where shape1 perhaps has more influence closer to 0 and shape2 closer to 1. The higher the number, the lower the density at its respective end of the distribution.\n\nrate &lt;- 10\n\nhist(rexp(10000, rate), main = \"exponential distribution\", probability = TRUE)\n\nx &lt;- seq(0, 1, length.out = 100)\nlines(x, dexp(x, rate), lwd = 2)\n\n\n\n\nAs we saw from the plotting exp() earlier, the majority sit at a very low value, until it reaches the point of rapid growth, where a rapidly decreasing amount of numbers can be found at each range on the way up. Plotting the density seems to give a horizontally flipped shape. The rate (\\(λ\\)) controls how rapid it grows."
  },
  {
    "objectID": "numeric.html#binomial-distribution",
    "href": "numeric.html#binomial-distribution",
    "title": "2  Numeric vectors",
    "section": "2.6 Binomial distribution",
    "text": "2.6 Binomial distribution\nMarek gives a really nice example here of six-sided dice rolls to explain both binomial distribution and the different distribution function prefixes R uses; d*2, b*, q* and r*. So, binomial distribution gives us the probability distribution of successes in a sequence of tests/trials. As per the example, this could be a sequence of dice rolls with a particular face result counted as a success.\n\nn = 12 # Number of trials\np = 1/6 # Probability of success of rolling a particular face in a single roll\n\n# See how likely each number of success is, from the minimum 0 times to the max n times\nround(dbinom(0:n, n, p), 2)\n\n [1] 0.11 0.27 0.30 0.20 0.09 0.03 0.01 0.00 0.00 0.00 0.00 0.00 0.00\n\n\nIt shows 2 successes is just about the most likely (as we started from 0). Then we can check how likely it is to see a certain number of successes or fewer.\n\nround(pbinom(0:n, n, p), 2)\n\n [1] 0.11 0.38 0.68 0.87 0.96 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00\n\n\nIt’s very unlikely to roll more than 4 of the same face, so we see 96% probability to roll 4 or less times. By 6 rolls, it’s indistinguishable from guaranteed (at 2 decimal places), which - yeah - does make sense for a 1/6 chance. Sort of the inverse view, we can also check the smallest number of (maximum?) successes for a certain likelihood. Here we use the q (quantile) function to check by decile:\n\nqbinom(seq(0, 1, length.out = 11), n, p)\n\n [1]  0  0  1  1  2  2  2  3  3  4 12\n\n\nSo to reach at least 70% or 80% likelihood of rolling our number of successes or lower, that number would have to be 3, as per the 0.87 figure from the probability distribution. I am half following what’s going on 😅. Back to more familiar territory of random number generation, we can see how many successes we get across multiple sets of n attempts:\n\nrbinom(30, n, p)\n\n [1] 3 3 1 1 1 1 5 4 3 2 2 6 4 1 1 3 2 3 3 1 4 2 2 1 2 1 1 3 3 4"
  },
  {
    "objectID": "numeric.html#special-functions",
    "href": "numeric.html#special-functions",
    "title": "2  Numeric vectors",
    "section": "2.7 Special functions",
    "text": "2.7 Special functions\n2.3.5. Special functions\n\nThe Pochhammer symbol, \\((a)x​ =Γ(a+x)/Γ(a)\\), can be computed via a call to gsl::poch(a, x), i.e., the poch function from the gsl package:\n\n\nRead the documentation of the corresponding gsl_sf_poch function in the GNU GSL manual. And when you are there, do not hesitate to go through the list of all functions, including those related to statistics, permutations, combinations, and so forth.\n\nLet’s try it out\n\nlibrary(gsl)\n\npoch(10, 3:6)\n\n[1]    1320   17160  240240 3603600\n\n\nI mean, at least the R code works. Oh boy, I’m so in over my head here. Ok, a bit of a simpler definition of the gamma function is \\(Γ(x) = (x - 1)!\\). At least I know what factorial does, so I’ll work with this one. I think it’s appropriate for any positive integer x3.\n\ngamma(5)\n\n[1] 24\n\nfactorial(5 - 1)\n\n[1] 24\n\n\nI’m not quite sure what we’re being pointed to the GSL manual for, at least for gsl_sf_poch it basically gives the same definition Marek did. Sure I can see there are loads of functions in the manual, but honestly it’s a bit much for me right now and I don’t know where to look. Similarly help(gsl::poch) just a whole bunch of functions. Special functions, for people who know what they are and what to do with them. Not me 🤷 (yet?)."
  },
  {
    "objectID": "numeric.html#diff-and-cumsum",
    "href": "numeric.html#diff-and-cumsum",
    "title": "2  Numeric vectors",
    "section": "2.8 diff() and cumsum()",
    "text": "2.8 diff() and cumsum()\n2.4.4. Accumulating\n\ndiff can be considered an inverse to cumsum. It computes the iterated difference: subtracts the first two elements, then the second from the third one, the third from the fourth, and so on. In other words, diff(x) gives \\(y\\) such that \\(y_i =x_{i+1} − x_i\\).\n\n\ntriangular &lt;- c(0, 1, 3, 6, 10, 15, 21, 28, 36, 45)\n\ncumsum(triangular)\n\n [1]   0   1   4  10  20  35  56  84 120 165\n\ndiff(triangular)\n\n[1] 1 2 3 4 5 6 7 8 9\n\n\nIndeed the difference goes up by one each time. Now, I want to try on a sequence based on iterated difference.\n\nfibonacci &lt;- c(1, 1, 2, 3, 5, 8, 13, 21, 34)\n\ncumsum(fibonacci)\n\n[1]  1  2  4  7 12 20 33 54 88\n\ndiff(fibonacci)\n\n[1]  0  1  1  2  3  5  8 13\n\n\nAha, as one might expect, diff gives us much of the original sequence."
  },
  {
    "objectID": "numeric.html#geometric-and-harmonic-mean",
    "href": "numeric.html#geometric-and-harmonic-mean",
    "title": "2  Numeric vectors",
    "section": "2.9 Geometric and Harmonic mean",
    "text": "2.9 Geometric and Harmonic mean\n\nLet \\(x\\) be any vector of length \\(n\\) with positive elements. Compute its geometric and harmonic mean, which are given by, respectively,\n\n\n\\(\\sqrt[n]{\\Pi_{i=1}^{n}x_i = e^{\\frac{1}{n} \\sum_{i=1}^{n} logx_i}}\\) and \\(\\frac{n}{\\sum_{i=1}^{n} \\frac{1}{x_i}}\\)\n\nI rewrote that out in TeX math, which has probably helped me follow a bit better. Harmonic mean looks simpler, so I’ll try that first. The length of a vector divided by the sum of one of each element of the vector, right?\n\nharmonic_mean &lt;- function(x) length(x) / sum(1/x)\n\nIs - is that it? Holy vectorisation! Not that this is a proper test, but let’s see, the harmonic mean of 1, 4, and 4 should be 2.\n\nharmonic_mean(c(1, 4, 4))\n\n[1] 2\n\n\nNice. Ok, geometric mean time. It looks like the exponential of (1 over the length of the vector, multiplied by the sum of the log of each element).\n\ngeometric_mean &lt;- function(x) exp(1/length(x) * sum(log(x)))\n\nSo I see that \\(\\sum_{i=1}^n\\) really can just be quickly read as the sum() across the vector. The geometric mean of 2 and 8 should be 4:\n\ngeometric_mean(c(2, 8))\n\n[1] 4"
  },
  {
    "objectID": "numeric.html#exercises",
    "href": "numeric.html#exercises",
    "title": "2  Numeric vectors",
    "section": "2.10 Exercises",
    "text": "2.10 Exercises\n2.5. Exercises\n\n*Answer the following questions.\nWhat is the meaning of the dot-dot-dot parameter in the definition of the c function?*\n\n... arguments signify that any number4 of arguments can be passed, in this case they are the objects for c() to attempt to combine.\n\nWe say that the round function is vectorised. What does that mean?\n\nWhen a numeric vector is passed to round() it will apply to every element of that vector. You don’t need for instance to loop through the vector, rounding each number individually. At least at the level of R code, as I assume round() is looping in a faster language under-the-hood.\n\nWhat is wrong with a call to c(sqrt(1), sqrt(2), sqrt(3))?\n\nAs sqrt() is vectorised, it doesn’t need to be called again for each number, instead you could rewrite this as sqrt(c(1, 2, 3)). This would make a huge difference for bigger and bigger vectors.\n\nWhat do we mean by saying that multiplication operates element by element?\n\nWhen you multiply two vectors, the result will be a vector the length of the longest input vector. Each element of the result vector is the result of the individual operation of multiplying the respective elements of the input vector. Something like for vectors \\(z = x * y\\) their elements are as $z_i = x_i * y_i$\n\nHow does the recycling rule work when applying +?\n\nWhen summing two vectors, if one vector is shorter than the other, the shorter one will be repeated until it is the same length e.g. 1:10 + 1 is equivalent to 1:10 + rep(1, 10). It will happen even if the longer object length is not a multiple of shorter object length, but will throw a warning to let you know.\n\nHow to (and why) set the seed of the pseudorandom number generator?\n\nUse set.seed() as I did earlier. This is done to make sure the results are reproducible, by yourself and others, later.\n\nWhat is the difference between NA_real_ and NaN?\n\nNA_real_ denotes a missing or unknown number in a numeric vector. NaN signifies an impossible operation on numeric values has been attempted, like taking the principal square root of a negative number.\n\n# Unknown\nsqrt(NA_real_)\n\n[1] NA\n\n# Impossible\nsqrt(-100)\n\nWarning in sqrt(-100): NaNs produced\n\n\n[1] NaN\n\n\n\nHow are default arguments specified in the manual of, e.g., the round function?\n\nThey are specified in the Usage section with the form function(named_argument = default_value). The usage round(x, digits = 0) shows that by default the result of rounding will have no decimal places.\n\nIs a call to rep(times=4, x=1:5) equivalent to rep(4, 1:5)?\n\nNo, because based on positional argument matching, rep(4, 1:5) is instead equivalent to rep(x = 4, times = 1:5). This is not only different, but invalid because times must either be the same length as x, or have a length of 1.\n\nList a few ways to generate a sequence like \\((-1, -0.75, -0.5, ..., 0.75, 1)\\).\n\nBy gaps of 0.25\n\nseq(-1, 1, 0.25)\n\n[1] -1.00 -0.75 -0.50 -0.25  0.00  0.25  0.50  0.75  1.00\n\n\nBy the same length of sequence\n\nseq(-1, 1, length.out = (9))\n\n[1] -1.00 -0.75 -0.50 -0.25  0.00  0.25  0.50  0.75  1.00\n\n\nAlong with a sequence of the same length\n\nseq(-1, 1, along.with = 1:9)\n\n[1] -1.00 -0.75 -0.50 -0.25  0.00  0.25  0.50  0.75  1.00\n\n\nAnd noting the default already is to = 1\n\nseq(-1, by = 0.25)\n\n[1] -1.00 -0.75 -0.50 -0.25  0.00  0.25  0.50  0.75  1.00\n\n\nOr do something worse like this?\n\ndiff = -1 - 1\nstep_size = 0.25\nsteps = abs(diff / step_size)\nvalues = -1 + (seq_len(steps) * step_size)\nc(-1, values)\n\n[1] -1.00 -0.75 -0.50 -0.25  0.00  0.25  0.50  0.75  1.00\n\n\n\nIs -3:5 the same as -(3:5)? What about the precedence of operators in expressions such as2^3/45^6,56+4/17%%8, and1+-2^3:4-1`?\n\n\n-3:5\n\n[1] -3 -2 -1  0  1  2  3  4  5\n\n-(3:5)\n\n[1] -3 -4 -5\n\n\nAre different because in the first one the start of the sequence is set to negative 3 whereas in the second one the whole sequence is set to negative.\nIn 2^3/4*5^6 taking the power of (^) comes first, then the the division and multipilcation is read in order. It can be understood as ((2^3)/4)*(5^6) as the division comes before the multiplication i.e. it’s not equivalent to (2^3)/(4*(5^6)).\nIn 5*6+4/17%%8 it’s read in the order %%, *, /, + i.e. as ((5*6)+4)/(17%%8).\nIn 1+-2^3:4-1 it’s read ^, unary -2, :, binary +, binary - i.e. as 1 + (-(2^3)):4 - 1, noting the binary + and - are outside the reach of the binary :.\n\nIf \\(x\\) is a numeric vector of length \\(n\\) (for some \\(n≥0\\)), how many values will sample(x) output?\n\nSample will return \\(n\\) values. If no size argument is passed to sample() it will pass length(x) as the size argument to sample.int().\n\nDoes scan support reading directly from compressed archives, e.g., .csv.gz files?\n\nYes, in help(scan), the file argument explicitly states that this can be a compressed file. I assume this example would be covered by gzfile()\n\nWhen in doubt, refer back to the material discussed in this chapter or the R manual."
  },
  {
    "objectID": "numeric.html#add-a-legend-to-your-plot",
    "href": "numeric.html#add-a-legend-to-your-plot",
    "title": "2  Numeric vectors",
    "section": "2.11 Add a legend() to your plot()",
    "text": "2.11 Add a legend() to your plot()\n\nThanks to vectorisation, implementing an example graph of arcsine and arccosine is straightforward.\n\n\nThusly inspired, plot the following functions: \\(|sinx^2|\\), \\(|sin|x||\\), \\(\\sqrt{⌊x⌋}\\) and \\(1() 1 + e^{-x})\\) Recall that the documentation of plot can be accessed by calling help(“plot.default”).\n\n\nx &lt;- seq(-1, 1, length.out=101) \n\nfuns &lt;- list(\n  function(x) abs(sin(x^2)),\n  function(x) abs(sin(abs(x))),\n  function(x) sqrt(floor(x)),\n  function(x) 1/(1+exp(-x))\n)\n\nplot(x, funs[[1]](x), \n     type = \"l\", lwd = 2,\n     ylab = \"four functions of x?\")\nlines(x, funs[[2]](x), col = \"red\", lwd = 2, lty = 2)\n# Oops I'm calling sqrt() on negative numbers\nlines(x, funs[[3]](x), col = \"blue\", lwd = 2, lty = 3)\n\nWarning in sqrt(floor(x)): NaNs produced\n\nlines(x, funs[[4]](x), col=\"purple\", lwd = 2, lty = 4)\n\nlegend(\n  \"bottomleft\", \n  vapply(funs, \\(x) deparse(body(x)), character(1)),\n  lty = 1:4,\n  col = c(\"black\", \"red\", \"blue\", \"purple\")\n)\n\n\n\n\nI don’t know if I did it. I certainly tried. Were they supposed to be separate? Did I interpret the notation correctly?"
  },
  {
    "objectID": "numeric.html#vectorise-over-long-vectors",
    "href": "numeric.html#vectorise-over-long-vectors",
    "title": "2  Numeric vectors",
    "section": "2.12 Vectorise over long vectors",
    "text": "2.12 Vectorise over long vectors\n\nThe expression: \\[\n4\\sum_{i=1}^{n} \\frac{(-1)^{i+1}}{2i- 1} =  4(\\frac{1}{1}-\\frac{1}{3}+\\frac{1}{5}-\\frac{1}{7}+...)\n\\] slowly converges to \\(π\\) as \\(n\\) approaches \\(∞\\). Calculate it for \\(n = 1,000,000\\) and \\(n = 1,000,000,000\\), using the vectorised functions and operators discussed in this chapter, making use of the recycling rule as much as possible.\n\n\nalmost_pi &lt;- function(n) {\n  x &lt;- 1:n\n  \n  4 * sum(((-1)^(x + 1) / ((2 * x) - 1))\n    )\n}\n\nalmost_pi(1e6)\n\n[1] 3.141592\n\n\n\nalmost_pi(1e9)\n\n\n\n[1] 3.141593\n\n\nStarting to get a feel for this vectorisation thing. I think I’m in the right track when I’m working with \\(x\\) and not with \\(i\\) e.g. not for (i in 1:n) {...}. However, for \\(n = 1,000,000,000\\) it still did take quite some time."
  },
  {
    "objectID": "numeric.html#pearson-linear-correlation",
    "href": "numeric.html#pearson-linear-correlation",
    "title": "2  Numeric vectors",
    "section": "2.13 Pearson linear correlation",
    "text": "2.13 Pearson linear correlation\n\n\nLet x and y be two vectors of identical lengths \\(n\\), say: *\n\n\n\nx &lt;- rnorm(100)\ny &lt;- 2*x+10+rnorm(100, 0, 0.5)\n\n\nCompute the Pearson linear correlation coefficient given by:\n\n\n\\[\nr = \\frac{\\sum_{i=1}^n(x_i - \\frac{1}{n} \\sum_{j=1}^{n}x_j)(y_i - \\frac{1}{n}\\sum_{j=1}^n y_j)}\n{\\sqrt{\\sum_{i=1}^n(x_i - \\frac{1}{n} \\sum_{j=1}^n x_j)^2}\n\\sqrt{(y_i - \\frac{1}{n}\\sum_{j=1}^n y_j)^2}}\n\\]\n\nOk, calm down. Think in vectors. Think in vectors. Break it up. The four bracketed groups look to have the same structure, one in each half of the fraction for x, one for y. Set that out, then just apply sum(), sqrt() and ^2 as necessary. Fairly simple, after all.\n\npearson_coeff &lt;- function(x, y) {\n  n = length(x)\n  stopifnot(n == length(y))\n  \n  x_group &lt;- x - (1/n) * sum(x)\n  y_group &lt;- y - (1/n) * sum(y)\n  \n  sum(x_group * y_group) / (sqrt(sum(x_group^2)) * sqrt(sum(y_group^2)))\n}\n\npearson_coeff(x, y)\n\n[1] 0.9657844\n\n\n\nTo make sure you have come up with a correct implementation, compare your result to a call to ‘cor(x, y)’.\n\n\ncor(x, y)\n\n[1] 0.9657844\n\n\nSuccess!"
  },
  {
    "objectID": "numeric.html#rolling-averages",
    "href": "numeric.html#rolling-averages",
    "title": "2  Numeric vectors",
    "section": "2.14 Rolling averages",
    "text": "2.14 Rolling averages\n\nFind an R package providing a function to compute moving (rolling) averages and medians of a given vector. Apply them on the EUR/AUD currency exchange data. Draw thus obtained smoothened versions of the time series.\n\nLooks like the package {zoo} has got it covered.\n\nlibrary(zoo)\n\n\nAttaching package: 'zoo'\n\n\nThe following objects are masked from 'package:base':\n\n    as.Date, as.Date.numeric\n\neur_to_aud &lt;- scan(paste0(\"https://github.com/gagolews/teaching-data/raw/\",\n    \"master/marek/euraud-20200101-20200630.csv\"), comment.char=\"#\") \n\neur_to_aud &lt;- na.omit(eur_to_aud)\n  \nrolling_mean &lt;- zoo::rollmean(eur_to_aud, 9)\n\nplot(rolling_mean, ylab = \"[EUR/AUD] rolling mean\")\n\n\n\nrolling_median &lt;- zoo::rollmedian(eur_to_aud, 9)\n\nplot(rolling_median, ylab = \"[EUR/AUD] rolling median\")"
  },
  {
    "objectID": "numeric.html#k-moving-average",
    "href": "numeric.html#k-moving-average",
    "title": "2  Numeric vectors",
    "section": "2.15 k-moving average",
    "text": "2.15 k-moving average\n…"
  },
  {
    "objectID": "numeric.html#footnotes",
    "href": "numeric.html#footnotes",
    "title": "2  Numeric vectors",
    "section": "",
    "text": "I found that while R does provide mean() and median(), the function mode() is more akin to typeof() in providing the data type/mode. There is not a standard in-built function for the statistical mode.↩︎\nAmusing to discover df() is a distribution function in R, after seeing so many dataframes borrowing that object name.↩︎\nOoh, ooh, let me try set notation: \\(x ∈ N\\)?↩︎\nWell I’m sure there IS an upper limit, but “any” should be acceptable here.↩︎"
  },
  {
    "objectID": "logical.html#simplifying-predicates",
    "href": "logical.html#simplifying-predicates",
    "title": "3  Logical vectors",
    "section": "3.1 Simplifying predicates",
    "text": "3.1 Simplifying predicates\n3.3.5. Simplifying predicates\n\nAssuming that a, b, and c are numeric vectors, simplify the following expressions:\n\n\n!(b&gt;a & b&lt;c),\n\nOne of the sub-conditions has to be FALSE for the result to be TRUE:\n!b&gt;a | !b&lt;c\n\n!(a&gt;=b & b&gt;=c & a&gt;=c),\n\nSame again:\n!a&gt;=b | !b&gt;=c | !a&gt;=c\n\na&gt;b & a&lt;c | a&lt;c & a&gt;d,\n\nI think I complicated rather than simplified, but:\na&lt;c & (a&lt;b | a&gt;d)\n\na&gt;b | a&lt;=b,\n\nUhh, TRUE?\n\na&lt;=b & a&gt;c | a&gt;b & a&lt;=c,\n\nHmm, not sure this one can be simplified.\n\na&lt;=b & (a&gt;c | a&gt;b) & a&lt;=c,\n\na is less than or equal to b AND less than or equal to c, but must be greater than at least one of b OR c, so always: FALSE.\n\n!all(a &gt; b & b &lt; c).\n\nIf all(p) is equivalent to !any(!p) then isn’t !all(p) equal to any(!p)?\nany(!a&gt;b | !b&lt;c)"
  },
  {
    "objectID": "logical.html#nesting-ifelse",
    "href": "logical.html#nesting-ifelse",
    "title": "3  Logical vectors",
    "section": "3.2 Nesting ifelse",
    "text": "3.2 Nesting ifelse\n3.4. Choosing elements with ifelse"
  },
  {
    "objectID": "logical.html#ifelse-to-generate-guassian-mixtures",
    "href": "logical.html#ifelse-to-generate-guassian-mixtures",
    "title": "3  Logical vectors",
    "section": "3.3 ifelse() to generate Guassian mixtures",
    "text": "3.3 ifelse() to generate Guassian mixtures\n\nwe generated a variate from the normal distribution that has the expected value of −2 with probability 20%, and from the one with the expectation of 3 otherwise. Thus inspired, generate the Gaussian mixtures:\n\n\n\\(\\frac{2}{3}X + \\frac{1}{3}Y\\), where \\(X ~ N(100, 16)\\) and \\(Y ~ N(116, 8)\\)\n\n\nn &lt;- 10000\n\nz &lt;- ifelse(runif(n) &lt;= (2/3), rnorm(n , 100, 16), rnorm(n, 116, 8))\n\nhist(z, breaks = 101, probability = TRUE, main = \"\", col = \"white\")\n\n\n\n\n\n\\(0.3X + 0.4Y + 0.3Z\\), where \\(X\\) ~ \\(N(-10, 2)\\), \\(Y\\) ~ \\(N(0,2)\\), and $Z $ ~ \\(N(10,2)\\)\n\n\nr &lt;- runif(n)\n\nd &lt;- ifelse(\n  r &lt;= 0.2,\n  rnorm(n, -10, 2),\n  ifelse(r &lt;= 0.7, rnorm(n, 0, 2), rnorm(n, 10, 2))\n)\n\nhist(d, breaks = 101, probability = TRUE, main = \"\", col = \"white\")"
  },
  {
    "objectID": "logical.html#exercises",
    "href": "logical.html#exercises",
    "title": "3  Logical vectors",
    "section": "3.4 Exercises",
    "text": "3.4 Exercises\n3.5. Exercises\n\nAnswer the following questions.\nWhy the statement “The Earth is flat or the smallpox vaccine is proven effective” is obviously true?\n\nBecause at least one of those two statements is true, which is all you need with an OR.\n\nWhat is the difference between NA and NA_real_?\n\nNA is of type logical wheras NA_real_ is of type numeric, i.e. they indicate a missing value within those respective vector types.\n\nWhy is “FALSE & NA” equal to FALSE, but “TRUE & NA” is NA?\n\nAs long as one side of an & is FALSE, the expression will evaluate to FALSE. With “TRUE & NA” the expression could evaluate to either TRUE or FALSE depending on what the NA value is.\n\nWhy has ifelse(x&gt;=0, sqrt(x), NA_real_) a tendency to generate warnings and how to rewrite it so as to prevent that from happening?\n\n\nWhat is the interpretation of mean(x &gt;= 0 & x &lt;= 1)?\n\nThis calculates the proportion of vector x which lies between 0 and 1. mean coerces the logical result of the expression to numeric i.e. TRUE to 1 and FALSE to 0.\n\nFor some integer x and y, how to verify whether 0&lt;x&lt;100, 0&lt;y&lt;100, and x&lt;y, all at the same time?\n\n0&lt;x & x&lt;y & y&lt;100\n\nMathematically, for all real \\(x,y&gt;0\\), we have \\(logxy=logx+logy\\). Why then all(log(x*y) == log(x)+log(y)) can sometimes return FALSE? How to fix this?\n\nDifferences can occur in the results due to over or under accuracy in calculations using the double-precision floating point system.\n\nIs x/y/z always equal to x/(y/z)? How to fix this?\n\n\n# 1 / 0.1\n0.1 / 0.1 / 0.1\n\n[1] 10\n\n# 0.1 / 1\n0.1 / (0.1 / 0.1)\n\n[1] 0.1\n\n\nClearly not as a result of order of operations. I’m not sure what is meant by fixing this.\n\nWhat is the purpose of very specific functions such as log1p and expm1 (see their help page) and many others listed in, e.g., the GNU GSL library? Is our referring to them a violation of the beloved “do not multiply entities without necessity” rule?\n\nHmm. Well it covers a large number of common mathematical operations. In a certain sense it does some to conflict with the idea of being independent in the R environment. On the other hand it makes a lot of sense to me to rely on more libraries which are, presumably, thoroughly tested and more robust then trying to replicate all these maths functions. Using them makes it easy to write simpler vectorised R code and I’m sure it’s more efficient having the iteration at the C/C++ level than the R level.\n\nIf we know that x may be subject to error, how to test whether x&gt;0 in a robust manner?\n\nWe can for instance use abs(x - 0) &lt; e-8 to check for within an assumed error margin or more simply round round(x) &gt; 0).\n\nIs “y&lt;-5” the same as “y &lt;- 5” or rather “y &lt; -5”?\n\nIt’s the same as the former, evaluated as assignment rather than less than negative 5."
  },
  {
    "objectID": "logical.html#equality-functions",
    "href": "logical.html#equality-functions",
    "title": "3  Logical vectors",
    "section": "3.5 Equality functions",
    "text": "3.5 Equality functions\n\nWhat is the difference between all and isTRUE? What about ==, identical, and all.equal? Is the last one properly vectorised?\n\nall checks if every element of a vector is TRUE, including coercion to TRUE, and isTRUE checks if the value passed to it is equal to the value TRUE.\n\n# Are all values TRUE?\nall(c(TRUE, TRUE))\n\n[1] TRUE\n\n# Is it a TRUE value?\nisTRUE(c(TRUE, TRUE))\n\n[1] FALSE\n\n\n== checks for equality between each respective element of two vectors, return a result for each pair. identical checks if two objects are exactly equal, a single result for whether the entire thing is the same. all.equal() is similar to identical, but instead tests for near equality.\n\nc(1, 2, 3) == c(1, 2, 3)\n\n[1] TRUE TRUE TRUE\n\nidentical(c(1, 2, 3), c(1, 2, 3))\n\n[1] TRUE\n\nidentical(c(1, 2, 3), c(1.1, 2.1, 3.1))\n\n[1] FALSE\n\nall.equal(c(1, 2, 3), c(1.1, 2.1, 3.1), tolerance = 0.1)\n\n[1] TRUE"
  },
  {
    "objectID": "logical.html#entropy-loss",
    "href": "logical.html#entropy-loss",
    "title": "3  Logical vectors",
    "section": "3.6 Entropy loss",
    "text": "3.6 Entropy loss\n\nCompute the cross-entropy loss between a numeric vector p with values in the interval (0,1) and a logical vector y, both of length n (you can generate them randomly or manually, it does not matter, it is just an exercise):\n\n\n\\(L(p, y) = \\frac{1}{n} \\sum_{i=1}^{n}l_i\\)\n\n\nwhere\n\\[\n\\begin{equation}\nl_i = \\begin{cases}\n- log (p_i) & \\text{ if } y_i \\text{ is TRUE} \\\\\n- log (1 - p_i) & \\text{ if } y_i \\text{ is FALSE}\n\\end{cases}\n\\end{equation}\n\\]\n\n\ncross_entropy_loss &lt;- function(p, y) {\n  stopifnot(is.numeric(p))\n  stopifnot(is.logical(y))\n  \n  n &lt;- length(p)\n  \n  stopifnot(n == length(y))\n  \n  (1 / n) * sum(ifelse(y, -log(p), -log(1-p)))\n}\n\n\nInterpretation: in classification problems, \\(y_i ∈{FALSE,TRUE}\\) denotes the true class of the i-th object (say, whether the i-th hospital patient is symptomatic) and \\(p_i ∈(0,1)\\) is a machine learning algorithm’s confidence that i belongs to class TRUE (e.g., how sure a decision tree model is that the corresponding person is unwell). Ideally, if \\(y_i\\) is TRUE, \\(p_i\\) should be close to 1 and to 0 otherwise. The cross-entropy loss quantifies by how much a classifier differs from the omniscient one. The use of the logarithm penalises strong beliefs in the wrong answer.\n\nInteresting, so there should be no loss here:\n\np &lt;- c(rep(0, 50), rep(1, 50))\ny &lt;- c(rep(FALSE, 50), rep(TRUE, 50))\n\ncross_entropy_loss(p, y)\n\n[1] 0\n\n\nThere should be some loss here, were particularly the middle of the vector will be “unsure” with \\(p_i\\) being far from the expected integer for the corresponding \\(y_i\\):\n\np &lt;- seq(0, 1, length.out = 100)\ny &lt;- as.logical(round(p))\n\ncross_entropy_loss(p, y)\n\n[1] 0.3037506\n\n\nAnd something completely off:\n\np &lt;- 1\ny &lt;- FALSE\n\ncross_entropy_loss(p, y)\n\n[1] Inf\n\n\nWe did it! Marek advises to make sure the exercises have been tackled with ifelse() and base R vectorised operations, as opposed to if and using indexing to work on individual elements of vectors. So far so good, and I like it, it really does make things simple but effective. 🫡👍"
  },
  {
    "objectID": "lists_attributes.html#cross-entropy-loss-take-two",
    "href": "lists_attributes.html#cross-entropy-loss-take-two",
    "title": "4  Lists and attributes",
    "section": "4.1 Cross-entropy loss take two",
    "text": "4.1 Cross-entropy loss take two\n\nIn one of the previous exercises, we computed the cross-entropy loss between a logical vector \\(y∈{0,1}^n\\) and a numeric vector \\(p ∈ (0,1)^n\\). This measure can be equivalently defined as:\n\\(L(p, y) = - \\frac{1}{n} \\left( \\sum_{i=1}^{n} y_i log(p_i) + (1 - y_i) log(1-p_i) \\right)\\)\nUsing vectorised operations, but not relying on ifelse this time, implement this formula\n\n\ncross_entropy_loss_2 &lt;- function(p, y) {\n  stopifnot(is.numeric(p))\n  stopifnot(is.logical(y))\n  \n  n &lt;- length(p)\n  \n  stopifnot(n == length(y))\n  \n  - (1/n) * sum( y * log(p) + (1 - y) * log(1 - p))\n}\n\n\nThen, compute the cross-entropy loss between, for instance, “y &lt;- sample(c(FALSE, TRUE), n)” and “p &lt;- runif(n)” for some n.\n\n\nn &lt;- 100\ny &lt;- sample(c(FALSE, TRUE), n, replace = TRUE)\np &lt;- runif(n)\n\ncross_entropy_loss_2(p, y)\n\n[1] 1.083705\n\n\n\nNote how seamlessly we translate between FALSE/TRUEs and 0/1s in the above equation (in particular, where \\(1 - y_i\\) means the logical negation of \\(y\\))."
  },
  {
    "objectID": "lists_attributes.html#first-attributes",
    "href": "lists_attributes.html#first-attributes",
    "title": "4  Lists and attributes",
    "section": "4.2 First attributes",
    "text": "4.2 First attributes\n\nCreate a list with EUR/AUD, EUR/GBP, and EUR/USD exchange rates read from the euraud-*.csv, eurgbp-*.csv, and eurusd-*.csv files in our data repository. Each of its three elements should be a numeric vector storing the currency exchange rates. Furthermore, equip them with currency_from, currency_to, date_from, and date_to attributes. For example:\n\n\ncurrency_to &lt;- c(\"AUD\", \"GBP\", \"USD\")\n\nstr(lapply(currency_to,\n  function(currency_to) {\n    data &lt;-\n      scan(\n        paste0(\n          \"https://github.com/gagolews/teaching-data/raw/\",\n          \"master/marek/eur\",\n          tolower(currency_to),\n          \"-20200101-20200630.csv\"\n        ),\n        comment.char = \"#\"\n      )\n    structure(\n      data,\n      currency_from = \"EUR\",\n      currency_to = currency_to,\n      date_from = \"2020-01-01\",\n      date_to = \"2020-06-30\"\n   )\n}))\n\nList of 3\n $ : num [1:182] NA 1.6 1.6 NA NA ...\n  ..- attr(*, \"currency_from\")= chr \"EUR\"\n  ..- attr(*, \"currency_to\")= chr \"AUD\"\n  ..- attr(*, \"date_from\")= chr \"2020-01-01\"\n  ..- attr(*, \"date_to\")= chr \"2020-06-30\"\n $ : num [1:182] NA 0.848 0.851 NA NA ...\n  ..- attr(*, \"currency_from\")= chr \"EUR\"\n  ..- attr(*, \"currency_to\")= chr \"GBP\"\n  ..- attr(*, \"date_from\")= chr \"2020-01-01\"\n  ..- attr(*, \"date_to\")= chr \"2020-06-30\"\n $ : num [1:182] NA 1.12 1.11 NA NA ...\n  ..- attr(*, \"currency_from\")= chr \"EUR\"\n  ..- attr(*, \"currency_to\")= chr \"USD\"\n  ..- attr(*, \"date_from\")= chr \"2020-01-01\"\n  ..- attr(*, \"date_to\")= chr \"2020-06-30\""
  },
  {
    "objectID": "lists_attributes.html#comment",
    "href": "lists_attributes.html#comment",
    "title": "4  Lists and attributes",
    "section": "4.3 Comment",
    "text": "4.3 Comment\n\ncomment is perhaps the most rarely used special attribute. Create an object (whatever) equipped with the comment attribute. Verify that assigning to it anything other than a character vector leads to an error. Read its value by calling the comment function. Display the object equipped with this attribute. Note that the print function ignores its existence whatsoever: this is how special it is.\n\n\nmy_numbers &lt;- c(1, 2, 3)\n\ncomment(my_numbers) &lt;- \"a curious attribute\"\n\n\n# Can't do this\ncomment(my_numbers) &lt;- TRUE\n\nError in `comment&lt;-`(`*tmp*`, value = TRUE): attempt to set invalid 'comment' attribute\n\n\n\ncomment(my_numbers)\n\n[1] \"a curious attribute\""
  },
  {
    "objectID": "lists_attributes.html#functions-which-return-named-vectors",
    "href": "lists_attributes.html#functions-which-return-named-vectors",
    "title": "4  Lists and attributes",
    "section": "4.4 Functions which return named vectors",
    "text": "4.4 Functions which return named vectors\n\nA whole lot of functions return named vectors. Evaluate the following expressions and read the corresponding pages in their documentation:\n\n\n\nquantile(runif(100)),\n\n\n\nquantile(runif(100))\n\n        0%        25%        50%        75%       100% \n0.01023927 0.24883099 0.51386585 0.76538435 0.98424252 \n\n\nNamed with the proportion at each quantile, neat.\n\n\nhist(runif(100), plot=FALSE),\n\n\n\nhist(runif(100), plot=FALSE)\n\n$breaks\n [1] 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0\n\n$counts\n [1] 12  8  7 10 12  4 15 10 10 12\n\n$density\n [1] 1.2 0.8 0.7 1.0 1.2 0.4 1.5 1.0 1.0 1.2\n\n$mids\n [1] 0.05 0.15 0.25 0.35 0.45 0.55 0.65 0.75 0.85 0.95\n\n$xname\n[1] \"runif(100)\"\n\n$equidist\n[1] TRUE\n\nattr(,\"class\")\n[1] \"histogram\"\n\n\nYes, a name for the different features of the plot. Breaks for the bins, counts for the frequency, you’ve got density and then one I wasn’t particularly expecting, mids for the middle of the breaks. A little bit of non-standard evaluation to store the x input as a name and then a settingn for using equal distances.\n\n\noptions() (take note of digits, scipen, max.print, and width),\n\n\n\noptions()[names(options()) %in% c(\"digits\", \"scipen\", \"max.print\", \"width\")]\n\n$digits\n[1] 7\n\n$max.print\n[1] 99999\n\n$scipen\n[1] 0\n\n$width\n[1] 80\n\n\nYeah I’ve used scipen before to see a full integer instead of the scientific 1.1e6 style version.\n\n\ncapabilities().\n\n\n\ncapabilities()\n\n       jpeg         png        tiff       tcltk         X11        aqua \n       TRUE        TRUE        TRUE        TRUE       FALSE       FALSE \n   http/ftp     sockets      libxml        fifo      cledit       iconv \n       TRUE        TRUE       FALSE        TRUE       FALSE        TRUE \n        NLS       Rprof     profmem       cairo         ICU long.double \n       TRUE        TRUE        TRUE        TRUE        TRUE        TRUE \n    libcurl \n       TRUE \n\n\nOk, info on whether my R build has certain functionality."
  }
]